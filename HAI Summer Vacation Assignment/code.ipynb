{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNrNn5rLhlnRCiqPAL0Iky7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AIEI1UM-62Y4","executionInfo":{"status":"ok","timestamp":1756372410721,"user_tz":-540,"elapsed":2820,"user":{"displayName":"김도영","userId":"14684136807762543565"}},"outputId":"6227fc78-230e-4279-cea3-f7534685d696"},"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","execution_count":27,"metadata":{"id":"9jOYaDXdRCyp","executionInfo":{"status":"ok","timestamp":1756372410722,"user_tz":-540,"elapsed":14,"user":{"displayName":"김도영","userId":"14684136807762543565"}}},"outputs":[],"source":["import torch.utils.data as data\n","import glob\n","import PIL\n","from torchvision.transforms import Compose, CenterCrop, Resize, ToTensor\n","\n","\n","def getCropImg(crop_size):\n","    return Compose([\n","        CenterCrop(crop_size),\n","        ToTensor()\n","    ])\n","\n","\n","def getLRimage(crop_size, upscale_factor):\n","    return Compose([\n","        CenterCrop(crop_size),\n","        Resize(crop_size // upscale_factor, PIL.Image.BICUBIC),\n","        Resize(crop_size, PIL.Image.BICUBIC),\n","        ToTensor()\n","    ])\n","\n","\n","class MyDataset(data.Dataset):\n","    def __init__(self, image_dir, start, end):\n","        super(MyDataset, self).__init__()\n","        self.png_files = glob.glob(image_dir + \"*.png\")\n","        self.png_files = sorted(self.png_files)\n","        self.png_files = self.png_files[start:end]\n","\n","        self.input_transform = getLRimage(64, 2)\n","        self.target_transform = getCropImg(64)\n","\n","    def __getitem__(self, index):\n","        input = PIL.Image.open(self.png_files[index])\n","        target = input.copy()\n","        if self.input_transform:\n","            input = self.input_transform(input)\n","        if self.target_transform:\n","            target = self.target_transform(target)\n","\n","        return input, target\n","\n","    def __len__(self):\n","        return len(self.png_files)\n"]},{"cell_type":"code","source":["import torch.nn as nn\n","\n","class SRCNN(nn.Module):\n","  def __init__(self):\n","    super(SRCNN, self).__init__()\n","\n","    self.layer1 = nn.Sequential(\n","      nn.Conv2d(3, 64, 9, stride = 1, padding = 4),\n","      nn.ReLU(inplace = True)\n","    )\n","    self.layer2 = nn.Sequential(\n","      nn.Conv2d(64, 32, 1, stride = 1, padding = 0),\n","      nn.ReLU(inplace = True)\n","    )\n","    self.layer3 = nn.Conv2d(32, 3, 5, stride = 1, padding = 2)\n","\n","  def forward(self, x):\n","\n","    x = self.layer1(x)\n","    x = self.layer2(x)\n","    x = self.layer3(x)\n","\n","    return x"],"metadata":{"id":"1rMNtOzITZkq","executionInfo":{"status":"ok","timestamp":1756372410724,"user_tz":-540,"elapsed":12,"user":{"displayName":"김도영","userId":"14684136807762543565"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.optim as optim\n","from torch.utils.data import DataLoader\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = SRCNN().to(device)\n","\n","epochs = 50\n","batch_size = 32\n","lr = 0.001\n","loss_fn = nn.MSELoss()\n","optimizer = optim.Adam(model.parameters(), lr=lr)\n","\n","trainDataNum, validDataNum, testDataNum = 363, 104, 52\n","train_set = MyDataset('/content/drive/MyDrive/DIV2K_519sampled/', 0, trainDataNum)\n","valid_set = MyDataset('/content/drive/MyDrive/DIV2K_519sampled/', trainDataNum, trainDataNum + validDataNum)\n","test_set = MyDataset('/content/drive/MyDrive/DIV2K_519sampled/', trainDataNum + validDataNum, trainDataNum + validDataNum + testDataNum)\n","\n","training_data_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True)\n","valid_data_loader = DataLoader(dataset=valid_set, batch_size=batch_size, shuffle=False)\n","test_data_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False)\n"],"metadata":{"id":"cWeguR3SX2Fk","executionInfo":{"status":"ok","timestamp":1756372410746,"user_tz":-540,"elapsed":29,"user":{"displayName":"김도영","userId":"14684136807762543565"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["for epoch in range(epochs):\n","  for i, batch in enumerate(training_data_loader):\n","    input, target = batch[0].to(device), batch[1].to(device)\n","    optimizer.zero_grad()\n","    output = model(input)\n","    loss = loss_fn(output, target)\n","    loss.backward()\n","    optimizer.step()\n","\n","  model.eval()\n","  val_loss = 0.0\n","  with torch.no_grad():\n","    for i, batch in enumerate(valid_data_loader):\n","      input, target = batch[0].to(device), batch[1].to(device)\n","      output = model(input)\n","      loss = loss_fn(output, target)\n","      val_loss += loss.item()\n","\n","  avg_val_loss = val_loss / len(valid_data_loader)\n","  print(f\"Epoch [{epoch+1}/{epochs}], Validation Loss: {avg_val_loss:.4f}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-Jt93f6ZSdG","executionInfo":{"status":"ok","timestamp":1756375577986,"user_tz":-540,"elapsed":3167238,"user":{"displayName":"김도영","userId":"14684136807762543565"}},"outputId":"5c60b7d1-6be4-43b5-bbaf-d7b464fb3cc2"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch [1/50], Validation Loss: 0.0382\n","Epoch [2/50], Validation Loss: 0.0217\n","Epoch [3/50], Validation Loss: 0.0137\n","Epoch [4/50], Validation Loss: 0.0108\n","Epoch [5/50], Validation Loss: 0.0086\n","Epoch [6/50], Validation Loss: 0.0069\n","Epoch [7/50], Validation Loss: 0.0060\n","Epoch [8/50], Validation Loss: 0.0051\n","Epoch [9/50], Validation Loss: 0.0046\n","Epoch [10/50], Validation Loss: 0.0043\n","Epoch [11/50], Validation Loss: 0.0040\n","Epoch [12/50], Validation Loss: 0.0038\n","Epoch [13/50], Validation Loss: 0.0037\n","Epoch [14/50], Validation Loss: 0.0038\n","Epoch [15/50], Validation Loss: 0.0036\n","Epoch [16/50], Validation Loss: 0.0034\n","Epoch [17/50], Validation Loss: 0.0033\n","Epoch [18/50], Validation Loss: 0.0032\n","Epoch [19/50], Validation Loss: 0.0032\n","Epoch [20/50], Validation Loss: 0.0031\n","Epoch [21/50], Validation Loss: 0.0031\n","Epoch [22/50], Validation Loss: 0.0030\n","Epoch [23/50], Validation Loss: 0.0030\n","Epoch [24/50], Validation Loss: 0.0029\n","Epoch [25/50], Validation Loss: 0.0030\n","Epoch [26/50], Validation Loss: 0.0029\n","Epoch [27/50], Validation Loss: 0.0028\n","Epoch [28/50], Validation Loss: 0.0028\n","Epoch [29/50], Validation Loss: 0.0028\n","Epoch [30/50], Validation Loss: 0.0029\n","Epoch [31/50], Validation Loss: 0.0029\n","Epoch [32/50], Validation Loss: 0.0029\n","Epoch [33/50], Validation Loss: 0.0026\n","Epoch [34/50], Validation Loss: 0.0026\n","Epoch [35/50], Validation Loss: 0.0026\n","Epoch [36/50], Validation Loss: 0.0026\n","Epoch [37/50], Validation Loss: 0.0026\n","Epoch [38/50], Validation Loss: 0.0030\n","Epoch [39/50], Validation Loss: 0.0037\n","Epoch [40/50], Validation Loss: 0.0028\n","Epoch [41/50], Validation Loss: 0.0026\n","Epoch [42/50], Validation Loss: 0.0025\n","Epoch [43/50], Validation Loss: 0.0025\n","Epoch [44/50], Validation Loss: 0.0025\n","Epoch [45/50], Validation Loss: 0.0025\n","Epoch [46/50], Validation Loss: 0.0024\n","Epoch [47/50], Validation Loss: 0.0025\n","Epoch [48/50], Validation Loss: 0.0024\n","Epoch [49/50], Validation Loss: 0.0024\n","Epoch [50/50], Validation Loss: 0.0024\n"]}]},{"cell_type":"code","source":["torch.save(model.state_dict(), \"srcnn_checkpoint.pth\")\n","\n","new_model = SRCNN().to(device)\n","new_model.load_state_dict(torch.load(\"srcnn_checkpoint.pth\"))\n","new_model.eval()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"c56V_SzSZSZv","executionInfo":{"status":"ok","timestamp":1756375578154,"user_tz":-540,"elapsed":170,"user":{"displayName":"김도영","userId":"14684136807762543565"}},"outputId":"25722a40-6ded-4562-cbed-965d59edea41"},"execution_count":31,"outputs":[{"output_type":"execute_result","data":{"text/plain":["SRCNN(\n","  (layer1): Sequential(\n","    (0): Conv2d(3, 64, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4))\n","    (1): ReLU(inplace=True)\n","  )\n","  (layer2): Sequential(\n","    (0): Conv2d(64, 32, kernel_size=(1, 1), stride=(1, 1))\n","    (1): ReLU(inplace=True)\n","  )\n","  (layer3): Conv2d(32, 3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",")"]},"metadata":{},"execution_count":31}]},{"cell_type":"code","source":["import os\n","import torchvision.transforms.functional as F\n","\n","save_dir = \"/content/SR_outputs/\"\n","os.makedirs(save_dir, exist_ok=True)\n","\n","with torch.no_grad():\n","  for idx, batch in enumerate(test_data_loader):\n","      inputs, targets = batch[0].to(device), batch[1].to(device)\n","      outputs = new_model(inputs)\n","\n","      for i in range(outputs.size(0)):\n","        output_img = outputs[i].cpu().clamp(0, 1)\n","        save_path = os.path.join(save_dir, f\"SR_output_{idx ^ test_data_loader.batch_size + i + 1}.png\")\n","        F.to_pil_image(output_img).save(save_path)"],"metadata":{"id":"rEVw3iYZKJI4","executionInfo":{"status":"ok","timestamp":1756376598230,"user_tz":-540,"elapsed":8853,"user":{"displayName":"김도영","userId":"14684136807762543565"}}},"execution_count":32,"outputs":[]}]}